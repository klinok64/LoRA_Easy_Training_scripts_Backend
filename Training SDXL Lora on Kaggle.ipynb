{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11049386,"sourceType":"datasetVersion","datasetId":6883421}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸŒŸ XL Lora Trainer by Hollowstrawberry\n\nColab Premium is recommended. Ideally you'd be changing the runtime to an A100 and use the maximum batch size, it will be several times faster.  \n\nThis colab is possible by open source code from talented people:\n* [kohya-ss](https://github.com/kohya-ss/sd-scripts)\n* [derrian-distro](https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/)\n* [Linaqruf](https://github.com/Linaqruf/kohya-trainer)\n* [Jelosus2](https://github.com/Jelosus2/LoRA_Easy_Training_Colab)","metadata":{"id":"rmCPmqFL6hCQ"}},{"cell_type":"markdown","source":"### â­• Disclaimer\nThe purpose of this document is to research bleeding-edge technologies in the field of machine learning.\nPlease read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html).","metadata":{"id":"vJ8clWTZEu-g"}},{"cell_type":"markdown","source":"| |GitHub|ğŸ‡¬ğŸ‡§ English|ğŸ‡ªğŸ‡¸ Spanish|\n|:--|:-:|:-:|:-:|\n| ğŸ  **Homepage** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n| ğŸ“Š **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n| â­ **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n| ğŸŒŸ **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) |  |\n| ğŸŒŸ **Legacy XL Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) |  |","metadata":{"id":"dPQlB4djNm3C"}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport toml\nfrom time import time\nfrom IPython.display import Markdown, display\nimport warnings\nimport shutil\n\nwarnings.filterwarnings(\"ignore\")\n\n# These carry information from past executions\nif \"model_url\" in globals():\n  old_model_url = model_url\nelse:\n  old_model_url = None\nif \"dependencies_installed\" not in globals():\n  dependencies_installed = False\nif \"model_file\" not in globals():\n  model_file = None\n\n# These may be set by other cells, some are legacy\nif \"custom_dataset\" not in globals():\n  custom_dataset = None\nif \"override_dataset_config_file\" not in globals():\n  override_dataset_config_file = None\nif \"continue_from_lora\" not in globals():\n  continue_from_lora = \"\"\nif \"override_config_file\" not in globals():\n  override_config_file = None\n\nLOWRAM = True\nLOAD_TRUNCATED_IMAGES = True\nBETTER_EPOCH_NAMES = True\nFIX_DIFFUSERS = True\n\n\n# ## ğŸš© Configuration Block - Start Here\n# Configure your training parameters in this section:\n\n# ### â–¶ï¸ Setup\n# Your project name will be the same as the folder containing your images. Spaces aren't allowed.\nproject_name = \"hypnohub1024\"\n# The folder structure doesn't matter and is purely for comfort. Make sure to always pick the same one. I like organizing by project.\nfolder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\"  # [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n# Decide the model that will be downloaded and used for training. You can also choose your own by pasting its download link, or a file in your Google Drive starting with `/content/drive/MyDrive`.\ntraining_model = \"Illustrious XL 0.1\"  # [\"Pony Diffusion V6 XL\", \"Illustrious XL 0.1\", \"NoobAI V-Pred 1.0\", \"NoobAI Eps 1.1\", \"Animagine XL V3\", \"Stable Diffusion XL 1.0 base\"]\noptional_custom_training_model = \"\"\ncustom_model_is_diffusers = False\ncustom_model_is_vpred = False\n# Use wandb if you want to visualize the progress of your training over time.\nwandb_key = \"\"\n\n# ### â–¶ï¸ Processing\n# Slider: min:512, max:1536, step:128\nresolution = 512\ncaption_extension = \".txt\"  # [\".txt\", \".caption\"]\n# Shuffling anime tags in place improves learning and prompting. An activation tag goes at the start of every text file and will not be shuffled.\nshuffle_tags = True\nshuffle_caption = shuffle_tags\n# [0,1,2,3]\nactivation_tags = \"0\"\nactivation_tags = int(activation_tags)\nkeep_tokens = activation_tags\n\n# ### â–¶ï¸ Steps\n# Your images will repeat this number of times during training. I recommend that your images multiplied by their repeats is around 100, or 1 repeat with more than 100 images.\nnum_repeats = 1\n# Choose how long you want to train for. A good starting point is around 10 epochs or around 2000 steps.\n# One epoch is a number of steps equal to: your number of images multiplied by their repeats, divided by batch size.\npreferred_unit = \"Epochs\"  # [\"Epochs\", \"Steps\"]\nhow_many = 5\nmax_train_epochs = how_many if preferred_unit == \"Epochs\" else None\nmax_train_steps = how_many if preferred_unit == \"Steps\" else None\n# Saving more epochs will let you compare your Lora's progress better.\nsave_every_n_epochs = 1\n# Slider: min:10, max:1000, step:10\n#save_every_n_steps = \"\"\nkeep_only_last_n_epochs = 10\nif not save_every_n_epochs:\n  save_every_n_epochs = max_train_epochs\nif not keep_only_last_n_epochs:\n  keep_only_last_n_epochs = max_train_epochs\n\n# ### â–¶ï¸ Learning\n# The learning rate is the most important for your results. If your Lora produces black images, lower the unet and text encoder to 1e-4 and 1e-5 respectively, or even lower.\n# If you're training a style you can choose to set the text encoder to 0.\nunet_lr = 3e-4\ntext_encoder_lr = 6e-5\n# The scheduler is the algorithm that guides the learning rate. If you're not sure, pick `constant` and ignore the number. I personally recommend `cosine_with_restarts` with 3 restarts.\nlr_scheduler = \"rex\"  # [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\", \"rex\"]\nlr_scheduler_number = 3\n# Steps spent \"warming up\" the learning rate during training for efficiency. I recommend leaving it at 5%.\n# Slider: min:0.0, max:0.2, step:0.01\nlr_warmup_ratio = 0.05\nlr_warmup_steps = 0\n# These settings may produce better results. min_snr_gamma adjusts loss over time. ip_noise_gamma adjusts random noise.\nmin_snr_gamma_enabled = False\n# Slider: min:4, max:16.0, step:0.5\nmin_snr_gamma = 8.0\nip_noise_gamma_enabled = True\n# Slider: min:0.05, max:0.1, step:0.01\nip_noise_gamma = 0.1\n# Multinoise may help with color balance (darker darks, lighter lights).\nmultinoise = True\n\n# ### â–¶ï¸ Structure\n# LoRA is the classic type and good for a variety of purposes. LoCon is good with artstyles as it has more layers to learn more aspects of the dataset.\nlora_type = \"LoCon\"  # [\"LoRA\", \"LoCon\", \"LoHa\"]\n# More dim means larger Lora, it can hold more information but more isn't always better.\n# Slider: min:1, max:32, step:1\nnetwork_dim = 24\n# Slider: min:1, max:32, step:1\nnetwork_alpha = 24\n# The following two values only apply to the additional layers of LoCon.\n# Slider: min:1, max:32, step:1\nconv_dim = 24\n# Slider: min:1, max:32, step:1\nconv_alpha = 24\n\n# ### â–¶ï¸ Training\n# Adjust these parameters depending on your colab configuration.\n# Higher batch size is often faster but uses more memory.\n# Slider: min:1, max:16, step:1\ntrain_batch_size = 6\n# I have found no substantial difference between sdpa and xformers.\ncross_attention = \"sdpa\"  # [\"sdpa\", \"xformers\"]\n# Use `full fp16` for lowest memory usage. The `bf16` options may not work correctly on the free GPU tier.\n# The Lora will be trained with the selected precision, but will always be saved in fp16 format for compatibility reasons.\nprecision = \"full fp16\"  # [\"float\", \"full fp16\", \"full bf16\", \"mixed fp16\", \"mixed bf16\"]\n# Caching latents to drive will add a 250KB file next to each image but will use considerably less memory.\ncache_latents = True\ncache_latents_to_drive = True\n# The following option will turn off shuffle_tags and disable text encoder training.\ncache_text_encoder_outputs = False\n\n# ### â–¶ï¸ Advanced\n# The optimizer is the algorithm used for training. AdamW8bit is the default and works great, while Prodigy manages learning rate automatically and may have several advantages such as training faster due to needing less steps as well as working better for small datasets.\noptimizer = \"Prodigy\"  # [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\", \"Came\"]\n# If Dadapt or Prodigy are selected and the recommended box is checked, the following values will override any previous settings:\n# `unet_lr=0.75`, `text_encoder_lr=0.75`, `network_alpha=network_dim`, `full_precision=False`\nrecommended_values = True\n# Alternatively set your own optimizer arguments separated by spaces (not commas). Recommended box must be disabled.\noptimizer_args = \"\"\n\n# ### â–¶ï¸ Ready - No changes needed below this line for basic configuration\n\n\nload_diffusers = False\nvpred = custom_model_is_vpred and len(optional_custom_training_model) > 0\n\nif optional_custom_training_model:\n  model_url = optional_custom_training_model\nelif \"Pony\" in training_model:\n  if load_diffusers:\n    model_url = \"https://huggingface.co/hollowstrawberry/67AB2F\"\n  else:\n    model_url = \"https://civitai.com/api/download/models/290640\"\n    model_file = \"ponyDiffusionV6XL.safetensors\"\nelif \"Animagine\" in training_model:\n  if load_diffusers:\n    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0\"\n  else:\n    model_url = \"https://civitai.com/api/download/models/293564\"\n    model_file = \"animagineXLV3.safetensors\"\nelif \"Illustrious\" in training_model:\n  if load_diffusers:\n    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0\"\n  else:\n    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n    model_file = \"Illustrious-XL-v0.1.safetensors\"\nelif \"NoobAI Eps\" in training_model:\n  if load_diffusers:\n    model_url = \"https://huggingface.co/Laxhar/noobai-XL-1.1\"\n  else:\n    model_url = \"https://huggingface.co/Laxhar/noobai-XL-1.1/resolve/main/NoobAI-XL-v1.1.safetensors\"\n    model_file = \"NoobAI-XL-v1.1.safetensors\"\nelif \"NoobAI V-Pred\" in training_model:\n  vpred = True\n  if load_diffusers:\n    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-1.0\"\n  else:\n    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-1.0/resolve/main/NoobAI-XL-Vpred-v1.0.safetensors\"\n    model_file = \"NoobAI-XL-Vpred-v1.0.safetensors\"\nelse:\n  if load_diffusers:\n    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\n  else:\n    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n    model_file = \"sd_xl_base_1.0.safetensors\"\n\nif load_diffusers:\n  vae_file = \"stabilityai/sdxl-vae\"\nelse:\n  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n  vae_file = \"sdxl_vae.safetensors\"\n\nmodel_url = model_url.strip()\n\n\nnetwork_module = \"networks.lora\"\nnetwork_args = None\nif lora_type.lower() == \"locon\":\n  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n\nif lora_type.lower() == \"loha\":\n  network_module = \"networks.loha\"\n  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n\n\nmixed_precision = \"no\"\nif \"fp16\" in precision:\n  mixed_precision = \"fp16\"\nelif \"bf16\" in precision:\n  mixed_precision = \"bf16\"\nfull_precision = \"full\" in precision\n\n\noptimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n\nif recommended_values:\n  if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n    unet_lr = 1\n    text_encoder_lr = 1\n    network_alpha = network_dim\n    full_precision = False\n  if optimizer == \"Prodigy\":\n    optimizer_args = [\"decouple=True\", \"weight_decay=0.01\", \"betas=[0.9,0.999]\", \"d_coef=2\", \"use_bias_correction=True\", \"safeguard_warmup=True\", \"slice_p=11\"]\n  elif optimizer == \"AdamW8bit\":\n    optimizer_args = [\"weight_decay=0.1\", \"betas=[0.9,0.99]\"]\n  elif optimizer == \"AdaFactor\":\n    optimizer_args = [\"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\"]\n  elif optimizer == \"Came\":\n    optimizer_args = [\"weight_decay=0.04\"]\n\nif optimizer == \"Came\":\n  optimizer = \"LoraEasyCustomOptimizer.came.CAME\"\n\nlr_scheduler_type = None\nlr_scheduler_args = None\nlr_scheduler_num_cycles = lr_scheduler_number\nlr_scheduler_power = lr_scheduler_number\n\nif \"rex\" in lr_scheduler:\n  lr_scheduler = \"cosine\"\n  lr_scheduler_type = \"LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts\"\n  lr_scheduler_args = [\"min_lr=1e-9\", \"gamma=0.9\", \"d=0.9\"]\n\n# Misc\nseed = 42\ngradient_accumulation_steps = 1\nbucket_reso_steps = 64\nmin_bucket_reso = 256\nmax_bucket_reso = 4096\n\n\n# ğŸ‘©â€ğŸ’» Cool code goes here\n\nroot_dir = \"/kaggle/working\"\ntrainer_dir = os.path.join(root_dir, \"trainer\")\nkohya_dir = os.path.join(trainer_dir, \"sd_scripts\")\n\nvenv_python = os.path.join(kohya_dir, \"venv/bin/python\")\nvenv_pip = os.path.join(kohya_dir, \"venv/bin/pip\")\ntrain_network = os.path.join(kohya_dir, \"sdxl_train_network.py\")\n\nif \"/Loras\" in folder_structure:\n  main_dir      = os.path.join(root_dir, \"Loras\")\n  log_folder    = os.path.join(main_dir, \"_logs\")\n  config_folder = os.path.join(main_dir, project_name)\n  output_folder = os.path.join(main_dir, project_name, \"output\")\nelse:\n  main_dir      = os.path.join(root_dir, \"lora_training\")\n  output_folder = os.path.join(main_dir, \"output\", project_name)\n  config_folder = os.path.join(main_dir, \"config\", project_name)\n  log_folder    = os.path.join(main_dir, \"log\")\n\n# Adjust images_folder to read from Kaggle Input datasets\nif folder_structure == \"Organize by project (MyDrive/Loras/project_name/dataset)\":\n    # Assuming dataset is uploaded to Kaggle as 'project_name_dataset'\n#    kaggle_dataset_name = f\"{project_name}\"\n#    input_images_folder = f\"/kaggle/working/Loras/{kaggle_dataset_name}/dataset\"\n    images_folder = os.path.join(main_dir, project_name, \"dataset\") # Working directory for dataset\n#elif folder_structure == \"Organize by category (MyDrive/lora_training/datasets/project_name)\":\n#    # Assuming dataset is uploaded to Kaggle as 'project_name_datasets'\n#    kaggle_dataset_name = f\"{project_name}\"\n#    input_images_folder = f\"/kaggle/working/Loras/{kaggle_dataset_name}/dataset/\"\n#    images_folder = os.path.join(root_dir, \"dataset\", project_name) # Working directory for dataset\n#else:\n#    input_images_folder = None # Should not happen, but for safety\n#    images_folder = None\n\nconfig_file = os.path.join(config_folder, \"training_config.toml\")\ndataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\naccelerate_config_file = \"/kaggle/working/.cache/huggingface/accelerate/default_config.yaml\"\n\n\ndef install_trainer():\n  print(\"âš ï¸ Installing trainer: This part might need internet access enabled in Kaggle Notebook settings if dependencies are not pre-installed in the environment.\")\n  # Kaggle environments often have many packages pre-installed.\n  # You might need to adapt this section based on Kaggle's pre-installed packages.\n  # For now, keeping similar installation commands, but be aware of potential issues.\n  !apt -y update -qq\n  !apt install -y python3.10-venv aria2 -qq\n  !{venv_pip} install numpy==1.26.4\n  !git clone https://github.com/klinok64/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n  !chmod 755 /kaggle/working/trainer/colab_install.sh\n  os.chdir(trainer_dir)\n  !./colab_install.sh\n  # %cd /kaggle/working/trainer\n  # !rm -f -r  sd_scripts\n  # !git clone https://github.com/kohya-ss/sd-scripts.git -b dev\n  # !mv sd-scripts sd_scripts\n  # %cd -\n\n  # fix logging\n  !{venv_pip} uninstall -y rich\n\n  # patch kohya\n  os.chdir(kohya_dir)\n  if LOAD_TRUNCATED_IMAGES:\n    !sed -i $'s/from PIL import Image/from PIL import Image, ImageFile\\\\\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py\n    if BETTER_EPOCH_NAMES:\n        !sed -i 's/{:06d}/{:02d}/g' library/train_util.py\n        !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py\n  if FIX_DIFFUSERS:\n    deprecation_utils = os.path.join(kohya_dir, \"venv/lib/python3.10/site-packages/diffusers/utils/deprecation_utils.py\")\n    deprecation_utils = os.path.join(kohya_dir, \"venv/lib/python3.10/site-packages/diffusers/utils/deprecation_utils.py\")\n    !sed -i 's/if version.parse/if False:#/g' {deprecation_utils}\n  if wandb_key:\n    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' train_network.py\n    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' sdxl_train.py\n\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n  os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n  os.chdir(root_dir)\n\nnew_input_folder = f\"/kaggle/input/{project_name}\"\nnew_working_folder = f\"/kaggle/working/Loras/{project_name}/dataset/\"\n\ndef copy_dataset_to_working(new_input_folder, new_working_folder):\n    if not new_input_folder or not new_working_folder:\n        print(\"Warning: Input or working dataset folder is not defined.\")\n        return False\n    if os.path.exists(new_working_folder):\n        print(f\"Dataset already exists in {new_working_folder}. Skipping copy.\")\n        return True\n    try:\n        print(f\"Copying dataset folder from {new_input_folder} to {new_working_folder}...\")\n        shutil.copytree(new_input_folder, new_working_folder)\n        print(\"Dataset copied successfully.\")\n        return True\n    except Exception as e:\n        print(f\"Error copying dataset \"f\"{project_name}\"f\": {e}\")\n        return False\n\n\ndef validate_dataset():\n  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, model_url\n  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n\n  if model_url.startswith(\"/kaggle/working/\") and not os.path.exists(model_url):\n    print(\"ğŸ’¥ Error: The custom training model you specified was not found in the working directory.\")\n    return\n\n  print(\"\\nğŸ’¿ Checking dataset...\")\n  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n    print(\"ğŸ’¥ Error: Please choose a valid project name.\")\n    return\n\n  # Find the folders and files\n  if custom_dataset:\n    try:\n      datconf = toml.loads(custom_dataset)\n      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n    except:\n      print(f\"ğŸ’¥ Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n      return\n    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n    folders = datasets_dict.keys()\n    files = [f for folder in folders for f in os.listdir(folder)]\n    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n  else:\n    reg = []\n    folders = [images_folder]\n    files = os.listdir(images_folder)\n    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n\n  # Validation - Adjust path messages\n  for folder in folders:\n    if not os.path.exists(folder):\n      print(f\"ğŸ’¥ Error: The folder {folder.replace('/kaggle/working/', '').replace('/kaggle/input/', '')} doesn't exist.\")\n      return\n  for folder, (img, rep) in images_repeats.items():\n    if not img:\n      print(f\"ğŸ’¥ Error: Your {main_dir}/{project_name} folder is empty.\")\n      return\n  # test_files = [] # Removed file name conflict check, might be needed later if errors occur\n  # for f in files:\n  #   if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n  #     print(f\"ğŸ’¥ Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n  #     return\n  #   for ff in test_files:\n  #     if f.endswith(supported_types) and ff.endswith(supported_types) \\\n  #         and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n  #       print(f\"ğŸ’¥ Error: The files {f} and {ff} cannot have the same name. Aborting.\")\n  #       return\n  #   test_files.append(f)\n\n  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n    caption_extension = \"\"\n  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n    print(f\"ğŸ’¥ Error: Invalid path to existing Lora. Example: /kaggle/working/Loras/example.safetensors\")\n    return\n\n  # Show estimations to the user\n\n  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n  estimated_epochs = int(total_steps/steps_per_epoch)\n  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n\n  for folder, (img, rep) in images_repeats.items():\n    print(\"ğŸ“\"+folder.replace(\"/kaggle/working/\", \"\").replace(\"/kaggle/input/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n    print(f\"ğŸ“ˆ Found {img} images with {rep} repeats, equaling {img*rep} steps.\")\n  print(f\"ğŸ“‰ Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n  if max_train_epochs:\n    print(f\"ğŸ”® There will be {max_train_epochs} epochs, for around {total_steps} total training steps.\")\n  else:\n    print(f\"ğŸ”® There will be {total_steps} steps, divided into {estimated_epochs} epochs and then some.\")\n\n  if total_steps > 30000:\n    print(\"ğŸ’¥ Error: Your total steps are too high. You probably made a mistake. Aborting...\")\n    return\n\n  return True\n\n\ndef create_config():\n  global dataset_config_file, config_file, model_file, vae_file\n  model_file = os.path.join(root_dir, model_file)\n  vae_file = os.path.join(root_dir, vae_file)\n  if override_config_file:\n    config_file = override_config_file\n    print(f\"\\nâ­• Using custom config file {config_file}\")\n  else:\n    config_dict = {\n      \"network_arguments\": {\n        \"unet_lr\": unet_lr,\n        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n        \"network_dim\": network_dim,\n        \"network_alpha\": network_alpha,\n        \"network_module\": network_module,\n        \"network_args\": network_args,\n        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n        \"network_weights\": continue_from_lora or None\n      },\n      \"optimizer_arguments\": {\n        \"learning_rate\": unet_lr,\n        \"lr_scheduler\": lr_scheduler,\n        \"lr_scheduler_type\": lr_scheduler_type,\n        \"lr_scheduler_args\": lr_scheduler_args,\n        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler not in (\"cosine\", \"constant\") else None,\n        \"optimizer_type\": optimizer,\n        \"optimizer_args\": optimizer_args or None,\n        \"loss_type\": \"l2\",\n        \"max_grad_norm\": 1.0,\n      },\n      \"training_arguments\": {\n        \"lowram\": LOWRAM,\n        \"pretrained_model_name_or_path\": model_file,\n        \"vae\": vae_file,\n        \"max_train_steps\": max_train_steps,\n        \"max_train_epochs\": max_train_epochs,\n        \"train_batch_size\": train_batch_size,\n        \"seed\": seed,\n        \"max_token_length\": 225,\n        \"xformers\": cross_attention == \"xformers\",\n        \"sdpa\": cross_attention == \"sdpa\",\n        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma_enabled else None,\n        \"ip_noise_gamma\": ip_noise_gamma if ip_noise_gamma_enabled else None,\n        \"no_half_vae\": True,\n        \"gradient_checkpointing\": True,\n        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n        \"max_data_loader_n_workers\": 1,\n        \"persistent_data_loader_workers\": True,\n        \"mixed_precision\": mixed_precision,\n        \"full_fp16\": mixed_precision == \"fp16\" and full_precision,\n        \"full_bf16\": mixed_precision == \"bf16\" and full_precision,\n        \"cache_latents\": cache_latents,\n        \"cache_latents_to_disk\": cache_latents_to_drive,\n        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n        \"min_timestep\": 0,\n        \"max_timestep\": 1000,\n        \"prior_loss_weight\": 1.0,\n        \"multires_noise_iterations\": 6 if multinoise else None,\n        \"multires_noise_discount\": 0.3 if multinoise else None,\n        \"v_parameterization\": vpred or None,\n        \"scale_v_pred_loss_like_noise_pred\": vpred or None,\n        \"zero_terminal_snr\": vpred or None,\n      },\n      \"saving_arguments\": {\n        \"save_precision\": \"fp16\",\n        \"save_model_as\": \"safetensors\",\n        \"save_every_n_epochs\": save_every_n_epochs,\n        \"save_last_n_epochs\": keep_only_last_n_epochs,\n        \"output_name\": project_name,\n        \"output_dir\": output_folder,\n        \"log_prefix\": project_name,\n        \"logging_dir\": log_folder,\n        \"wandb_api_key\": wandb_key or None,\n        \"log_with\": \"wandb\" if wandb_key else None,\n      }\n    }\n\n    for key in config_dict:\n      if isinstance(config_dict[key], dict):\n        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n\n    with open(config_file, \"w\") as f:\n      f.write(toml.dumps(config_dict))\n    print(f\"\\nğŸ“„ Config saved to {config_file}\")\n\n  if override_dataset_config_file:\n    dataset_config_file = override_dataset_config_file\n    print(f\"â­• Using custom dataset config file {dataset_config_file}\")\n  else:\n    dataset_config_dict = {\n      \"general\": {\n        \"resolution\": resolution,\n        \"shuffle_caption\": shuffle_caption and not cache_text_encoder_outputs,\n        \"keep_tokens\": keep_tokens,\n        \"flip_aug\": False,\n        \"caption_extension\": caption_extension,\n        \"enable_bucket\": True,\n        \"bucket_no_upscale\": False,\n        \"bucket_reso_steps\": bucket_reso_steps,\n        \"min_bucket_reso\": min_bucket_reso,\n        \"max_bucket_reso\": max_bucket_reso,\n      },\n      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n        {\n          \"subsets\": [\n            {\n              \"num_repeats\": num_repeats,\n              \"image_dir\": images_folder,\n              \"class_tokens\": None if caption_extension else project_name\n            }\n          ]\n        }\n      ]\n    }\n\n    for key in dataset_config_dict:\n      if isinstance(dataset_config_dict[key], dict):\n        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n\n    with open(dataset_config_file, \"w\") as f:\n      f.write(toml.dumps(dataset_config_dict))\n    print(f\"ğŸ“„ Dataset config saved to {dataset_config_file}\")\n\n\ndef download_model():\n  global old_model_url, model_url, model_file, vae_url, vae_file\n  real_model_url = model_url  # There was a reason for having a separate variable but I forgot what it was.\n\n  if real_model_url.startswith(\"/kaggle/working/\"):\n    # Local model, already checked to exist\n    model_file = real_model_url\n    print(f\"ğŸ“ Using local model file: {model_file}\")\n    # Validation\n    if model_file.lower().endswith(\".safetensors\"):\n      from safetensors.torch import load_file as load_safetensors\n      try:\n        test = load_safetensors(model_file)\n        del test\n      except:\n        return False\n    elif model_file.lower().endswith(\".ckpt\"):\n      from torch import load as load_ckpt\n      try:\n        test = load_ckpt(model_file)\n        del test\n      except:\n        return False\n    return True\n\n  else:\n    # Downloadable model\n#    if load_diffusers:\n#      if 'huggingface.co' in real_model_url:\n#          match = re.search(r'huggingface\\.co/([^/]+)/([^/]+)', real_model_url)\n#          if match:\n#              username = match.group(1)\n#              model_name = match.group(2)\n#              model_file = f\"{username}/{model_name}\"\n#              from huggingface_hub import HfFileSystem\n#              fs = HfFileSystem()\n#              existing_folders = set(fs.ls(model_file, detail=False))\n#              necessary_folders = [ \"scheduler\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"unet\", \"vae\" ]\n#              if all(f\"{model_file}/{folder}\" in existing_folders for folder in necessary_folders):\n#                print(\"ğŸƒ Diffusers model identified.\")  # Will be handled by kohya\n#                return True\n#      raise ValueError(\"ğŸ’¥ Failed to load Diffusers model. If this model is not Diffusers, have you tried turning it off at the top of the colab?\")\n\n    # Define local filename\n    if not model_file or old_model_url and old_model_url != model_url:\n      if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n        model_file = f\"{real_model_url[real_model_url.rfind('/'):]}\"\n      else:\n        model_file = \"downloaded_model.safetensors\"\n        if os.path.exists(model_file):\n          !rm \"{model_file}\"\n\n    # HuggingFace\n    if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", real_model_url):\n      real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n    # Civitai\n    elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", real_model_url):\n      if m.group(2):\n        model_file = f\"{m.group(2)}.safetensors\"\n      if m := re.search(r\"modelVersionId=([0-9]+)\", real_model_url):\n        real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n      else:\n        raise ValueError(\"ğŸ’¥ optional_custom_training_model contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n\n    # Download checkpoint\n    print({real_model_url})\n    print({model_file})\n    !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -o \"Illustrious-XL-v0.1.safetensors\"\n    model_file = os.path.join(root_dir, model_file)\n\n    # Download VAE\n    if not os.path.exists(vae_file):\n      !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d \"{root_dir}\" -o \"{vae_file}\"\n    vae_file = os.path.join(root_dir, vae_file)\n\n    # Validation\n\n    if model_file.lower().endswith(\".safetensors\"):\n      from safetensors.torch import load_file as load_safetensors\n      try:\n        test = load_safetensors(model_file)\n        del test\n      except:\n        new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n        !mv \"{model_file}\" \"{new_model_file}\"\n        model_file = new_model_file\n        print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n\n    if model_file.lower().endswith(\".ckpt\"):\n      from torch import load as load_ckpt\n      try:\n        test = load_ckpt(model_file)\n        del test\n      except:\n        return False\n\n  return True\n\n\ndef calculate_rex_steps():\n    # https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/blob/c34084b0435e6e19bb7a01ac1ecbadd185ee8c1e/utils/validation.py#L268\n    global max_train_steps\n\n    cache_file = os.path.join(config_folder, \"rex_steps_cache.json\")\n\n    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° ĞºĞµÑˆĞ° Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚\n    if os.path.exists(cache_file):\n        try:\n            with open(cache_file, \"r\") as f:\n                cached_data = json.load(f)\n                if cached_data.get(\"max_train_steps\") == max_train_steps and \\\n                   cached_data.get(\"max_train_epochs\") == max_train_epochs and \\\n                   cached_data.get(\"train_batch_size\") == train_batch_size and \\\n                   cached_data.get(\"lr_scheduler_num_cycles\") == lr_scheduler_num_cycles and \\\n                   cached_data.get(\"lr_warmup_ratio\") == lr_warmup_ratio and \\\n                   cached_data.get(\"dataset_config_file\") == dataset_config_file:\n\n                    calculated_max_steps = cached_data[\"calculated_max_steps\"]\n                    print(\"ğŸ¤” Rex steps loaded from cache\")\n\n                    cycle_steps = calculated_max_steps // (lr_scheduler_num_cycles or 1)\n                    print(f\"  cycle steps: {cycle_steps}\")\n                    lr_scheduler_args.append(f\"first_cycle_max_steps={cycle_steps}\")\n\n                    warmup_steps = round(calculated_max_steps * lr_warmup_ratio) // (lr_scheduler_num_cycles or 1)\n                    if warmup_steps > 0:\n                        print(f\"  warmup steps: {warmup_steps}\")\n                        lr_scheduler_args.append(f\"warmup_steps={warmup_steps}\")\n\n                    return\n\n        except Exception as e:\n            print(f\"âš ï¸ Error loading cache: {e}\")\n\n\n    print(\"\\nğŸ¤” Calculating Rex steps\")\n    if max_train_steps:\n        calculated_max_steps = max_train_steps\n    else:\n        from library.train_util import BucketManager\n        from PIL import Image\n        from pathlib import Path\n        import math\n\n        with open(dataset_config_file, \"r\") as f:\n            subsets = toml.load(f)[\"datasets\"][0][\"subsets\"]\n\n        supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"]\n        res = (resolution, resolution)\n        bucketManager = BucketManager(False, res, min_bucket_reso, max_bucket_reso, bucket_reso_steps)\n        bucketManager.make_buckets()\n        for subset in subsets:\n            for image in Path(subset[\"image_dir\"]).iterdir():\n                if image.suffix not in supported_types:\n                    continue\n                with Image.open(image) as img:\n                    bucket_reso, _, _ = bucketManager.select_bucket(img.width, img.height)\n                    for _ in range(subset[\"num_repeats\"]):\n                        bucketManager.add_image(bucket_reso, image)\n        steps_before_acc = sum(math.ceil(len(bucket) / train_batch_size) for bucket in bucketManager.buckets)\n        calculated_max_steps = math.ceil(steps_before_acc / gradient_accumulation_steps) * max_train_epochs\n        del bucketManager\n\n    cycle_steps = calculated_max_steps // (lr_scheduler_num_cycles or 1)\n    print(f\"  cycle steps: {cycle_steps}\")\n    lr_scheduler_args.append(f\"first_cycle_max_steps={cycle_steps}\")\n\n    warmup_steps = round(calculated_max_steps * lr_warmup_ratio) // (lr_scheduler_num_cycles or 1)\n    if warmup_steps > 0:\n        print(f\"  warmup steps: {warmup_steps}\")\n        lr_scheduler_args.append(f\"warmup_steps={warmup_steps}\")\n\n    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² Ñ„Ğ°Ğ¹Ğ» ĞºĞµÑˆĞ°\n    with open(cache_file, \"w\") as f:\n        json.dump({\"calculated_max_steps\": calculated_max_steps,\n                   \"max_train_steps\": max_train_steps,\n                   \"max_train_epochs\": max_train_epochs,\n                   \"train_batch_size\": train_batch_size,\n                   \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles,\n                   \"lr_warmup_ratio\": lr_warmup_ratio,\n                   \"dataset_config_file\": dataset_config_file}, f)\n    print(\"ğŸ¤” Rex steps calculated and cached\")\n\n\ndef main():\n  global dependencies_installed, images_folder\n\n  # Kaggle: No need to mount Google Drive\n  # if not os.path.exists('/content/drive'):\n  #   from google.colab import drive\n  #   print(\"ğŸ“‚ Connecting to Google Drive...\")\n  #   drive.mount('/content/drive')\n\n  new_working_folder = os.path.join(\"/kaggle/working/Loras/\", project_name, \"dataset\")\n\n  if not copy_dataset_to_working(new_input_folder, new_working_folder):\n      print(\"Failed to copy dataset. Aborting.\")\n      return\n\n  for dir in (main_dir, trainer_dir, log_folder, images_folder, output_folder, config_folder):\n    os.makedirs(dir, exist_ok=True)\n\n  if not validate_dataset():\n    return\n\n  if not dependencies_installed:\n    print(\"\\nğŸ­ Installing trainer...\\n\")\n    t0 = time()\n    install_trainer()\n    t1 = time()\n    dependencies_installed = True\n    print(f\"\\nâœ… Installation finished in {int(t1-t0)} seconds.\")\n  else:\n    print(\"\\nâœ… Dependencies already installed.\")\n\n  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n    print(\"\\nğŸ”„ Getting model...\")\n    if not download_model():\n      print(\"\\nğŸ’¥ Error: The model you specified is invalid or corrupted.\"\n            \"\\nIf you're using an URL, please check that the model is accessible without being logged in.\"\n            \"\\nYou can try civitai or huggingface URLs.\")\n      return\n    print()\n  else:\n    print(\"\\nğŸ”„ Model already downloaded.\\n\")\n\n  if lr_scheduler_type:\n    create_config()\n    os.chdir(kohya_dir)\n    calculate_rex_steps()\n    os.chdir(root_dir)\n\n  create_config()\n\n  print(\"\\nâ­ Starting trainer...\\n\")\n\n#  os.chdir(kohya_dir)\n  accelerate_script = os.path.join(kohya_dir, \"venv/bin/accelerate\")\n#  !{venv_python} {train_network} --config_file={config_file} --dataset_config={dataset_config_file} --debiased_estimation_loss\n  !{accelerate_script} launch --quiet --multi_gpu --gpu_ids all --num_processes 2 --num_cpu_threads_per_process 1 {train_network} --dataset_config={dataset_config_file} --config_file={config_file} --debiased_estimation_loss --ddp_gradient_as_bucket_view\n  #!{accelerate_script} launch --quiet --num_processes 1 --num_cpu_threads_per_process 1 {train_network} --dataset_config={dataset_config_file} --config_file={config_file} --debiased_estimation_loss\n  os.chdir(root_dir)\n\n  if not get_ipython().__dict__['user_ns']['_exit_code']:\n    display(Markdown(\"### âœ… Done! [Your Lora is in the Kaggle Working directory](https://www.kaggle.com/code/your_kaggle_username/your_notebook_slug/output)\\n\"\n                     \"### There will be several files, you should try the latest version (the file with the largest number next to it)\"))\n\nmain()","metadata":{"cellView":"form","id":"OglZzI_ujZq-","outputId":"85d43cab-b903-45e1-f43a-16405e37570e","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T21:54:29.960949Z","iopub.execute_input":"2025-03-16T21:54:29.961297Z","iopub.status.idle":"2025-03-17T04:07:24.074181Z","shell.execute_reply.started":"2025-03-16T21:54:29.961272Z","shell.execute_reply":"2025-03-17T04:07:24.073025Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Dataset already exists in /kaggle/working/Loras/hypnohub1024/dataset. Skipping copy.\n\nğŸ’¿ Checking dataset...\nğŸ“Loras/hypnohub1024/dataset/1\nğŸ“ˆ Found 2847 images with 1 repeats, equaling 2847 steps.\nğŸ“Loras/hypnohub1024/dataset/2\nğŸ“ˆ Found 4224 images with 1 repeats, equaling 4224 steps.\nğŸ“Loras/hypnohub1024/dataset/3\nğŸ“ˆ Found 2815 images with 1 repeats, equaling 2815 steps.\nğŸ“Loras/hypnohub1024/dataset/4\nğŸ“ˆ Found 1080 images with 1 repeats, equaling 1080 steps.\nğŸ“Loras/hypnohub1024/dataset/5\nğŸ“ˆ Found 315 images with 1 repeats, equaling 315 steps.\nğŸ“Loras/hypnohub1024/dataset/6\nğŸ“ˆ Found 88 images with 1 repeats, equaling 88 steps.\nğŸ“Loras/hypnohub1024/dataset/7\nğŸ“ˆ Found 23 images with 1 repeats, equaling 23 steps.\nğŸ“‰ Divide 11392 steps by 6 batch size to get 1898.6666666666667 steps per epoch.\nğŸ”® There will be 5 epochs, for around 9493 total training steps.\n\nâœ… Dependencies already installed.\n\nğŸ”„ Model already downloaded.\n\n\nğŸ“„ Config saved to /kaggle/working/Loras/hypnohub1024/training_config.toml\nğŸ“„ Dataset config saved to /kaggle/working/Loras/hypnohub1024/dataset_config.toml\n\nğŸ¤” Calculating Rex steps\n  cycle steps: 3173\n  warmup steps: 158\nğŸ¤” Rex steps calculated and cached\n\nğŸ“„ Config saved to /kaggle/working/Loras/hypnohub1024/training_config.toml\nğŸ“„ Dataset config saved to /kaggle/working/Loras/hypnohub1024/dataset_config.toml\n\nâ­ Starting trainer...\n\nError in sitecustomize; set PYTHONVERBOSE for traceback:\nModuleNotFoundError: No module named 'log'\nError in sitecustomize; set PYTHONVERBOSE for traceback:\nModuleNotFoundError: No module named 'log'\nError in sitecustomize; set PYTHONVERBOSE for traceback:\nModuleNotFoundError: No module named 'log'\nrich is not installed, using basic logging\nrich is not installed, using basic logging\nLoading settings from /kaggle/working/Loras/hypnohub1024/training_config.toml...\nLoading settings from /kaggle/working/Loras/hypnohub1024/training_config.toml...\n/kaggle/working/Loras/hypnohub1024/training_config\nrich is not installed, using basic logging\n/kaggle/working/Loras/hypnohub1024/training_config\nrich is not installed, using basic logging\nprepare tokenizers\nprepare tokenizers\nupdate token length: 225\nupdate token length: 225\nLoading dataset config from /kaggle/working/Loras/hypnohub1024/dataset_config.toml\nLoading dataset config from /kaggle/working/Loras/hypnohub1024/dataset_config.toml\nprepare images.\nprepare images.\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/1 contains 2847 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/1 contains 2847 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/2 contains 4224 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/2 contains 4224 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/3 contains 2815 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/3 contains 2815 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/4 contains 1080 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/4 contains 1080 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/5 contains 315 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/5 contains 315 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/6 contains 88 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/6 contains 88 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/7 contains 23 image files\nfound directory /kaggle/working/Loras/hypnohub1024/dataset/7 contains 23 image files\n11392 train images with repeating.\n11392 train images with repeating.\n0 reg images.\nno regularization images / æ­£å‰‡åŒ–ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\n0 reg images.\nno regularization images / æ­£å‰‡åŒ–ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\n[Dataset 0]\n  batch_size: 6\n  resolution: (512, 512)\n  enable_bucket: True\n  network_multiplier: 1.0\n  min_bucket_reso: 256\n  max_bucket_reso: 4096\n  bucket_reso_steps: 64\n  bucket_no_upscale: False\n\n  [Subset 0 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/1\"\n    image_count: 2847\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 1\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 1 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/2\"\n    image_count: 4224\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 2\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 2 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/3\"\n    image_count: 2815\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 3\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 3 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/4\"\n    image_count: 1080\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 4\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 4 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/5\"\n    image_count: 315\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 5\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 5 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/6\"\n    image_count: 88\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 6\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 6 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/7\"\n    image_count: 23\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 7\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n\n[Dataset 0]\n  batch_size: 6\n  resolution: (512, 512)\n  enable_bucket: True\n  network_multiplier: 1.0\n  min_bucket_reso: 256\n  max_bucket_reso: 4096\n  bucket_reso_steps: 64\n  bucket_no_upscale: False\n\n  [Subset 0 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/1\"\n    image_count: 2847\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 1\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 1 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/2\"\n    image_count: 4224\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 2\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 2 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/3\"\n    image_count: 2815\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 3\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 3 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/4\"\n    image_count: 1080\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 4\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 4 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/5\"\n    image_count: 315\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 5\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 5 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/6\"\n    image_count: 88\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 6\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n  [Subset 6 of Dataset 0]\n    image_dir: \"/kaggle/working/Loras/hypnohub1024/dataset/7\"\n    image_count: 23\n    num_repeats: 1\n    shuffle_caption: True\n    keep_tokens: 7\n    keep_tokens_separator: \n    caption_separator: ,\n    secondary_separator: None\n    enable_wildcard: False\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    caption_prefix: None\n    caption_suffix: None\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    alpha_mask: False,\n    is_reg: False\n    class_tokens: None\n    caption_extension: .txt\n\n\n[Dataset 0]\nloading image sizes.\n[Dataset 0]\nloading image sizes.\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11392/11392 [00:00<00:00, 48581.01it/s]\nmake buckets\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11392/11392 [00:00<00:00, 47826.61it/s]\nmake buckets\nnumber of images (including repeats) / å„bucketã®ç”»åƒæšæ•°ï¼ˆç¹°ã‚Šè¿”ã—å›æ•°ã‚’å«ã‚€ï¼‰\nbucket 0: resolution (256, 832), count: 23\nbucket 1: resolution (256, 960), count: 2\nbucket 2: resolution (256, 1024), count: 12\nbucket 3: resolution (320, 704), count: 269\nbucket 4: resolution (320, 768), count: 81\nbucket 5: resolution (384, 640), count: 1777\nbucket 6: resolution (448, 576), count: 5180\nbucket 7: resolution (512, 512), count: 1806\nbucket 8: resolution (576, 448), count: 1410\nbucket 9: resolution (640, 384), count: 648\nbucket 10: resolution (704, 320), count: 113\nbucket 11: resolution (768, 320), count: 57\nbucket 12: resolution (832, 256), count: 13\nbucket 13: resolution (960, 256), count: 1\nmean ar error (without repeats): 0.05830718499493897\nnumber of images (including repeats) / å„bucketã®ç”»åƒæšæ•°ï¼ˆç¹°ã‚Šè¿”ã—å›æ•°ã‚’å«ã‚€ï¼‰\nbucket 0: resolution (256, 832), count: 23\nbucket 1: resolution (256, 960), count: 2\nbucket 2: resolution (256, 1024), count: 12\nbucket 3: resolution (320, 704), count: 269\nbucket 4: resolution (320, 768), count: 81\nbucket 5: resolution (384, 640), count: 1777\nbucket 6: resolution (448, 576), count: 5180\nbucket 7: resolution (512, 512), count: 1806\nbucket 8: resolution (576, 448), count: 1410\nbucket 9: resolution (640, 384), count: 648\nbucket 10: resolution (704, 320), count: 113\nbucket 11: resolution (768, 320), count: 57\nbucket 12: resolution (832, 256), count: 13\nbucket 13: resolution (960, 256), count: 1\nmean ar error (without repeats): 0.05830718499493897\npreparing accelerator\npreparing accelerator\naccelerator device: cuda:0\nloading model for process 0/2\nload StableDiffusion checkpoint: /kaggle/working/Illustrious-XL-v0.1.safetensors\nbuilding U-Net\naccelerator device: cuda:1\n[rank1]:[W316 21:54:43.029259521 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\nloading U-Net from checkpoint\nU-Net: <All keys matched successfully>\nbuilding text encoders\nloading text encoders from checkpoint\ntext encoder 1: <All keys matched successfully>\ntext encoder 2: <All keys matched successfully>\nbuilding VAE\nloading VAE from checkpoint\nVAE: <All keys matched successfully>\nload VAE: /kaggle/working/sdxl_vae.safetensors\nadditional VAE loaded\n[rank0]:[W316 21:55:00.856337965 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\nloading model for process 1/2\nload StableDiffusion checkpoint: /kaggle/working/Illustrious-XL-v0.1.safetensors\nbuilding U-Net\nloading U-Net from checkpoint\nU-Net: <All keys matched successfully>\nbuilding text encoders\nloading text encoders from checkpoint\ntext encoder 1: <All keys matched successfully>\ntext encoder 2: <All keys matched successfully>\nbuilding VAE\nloading VAE from checkpoint\nVAE: <All keys matched successfully>\nload VAE: /kaggle/working/sdxl_vae.safetensors\nadditional VAE loaded\nEnable SDPA for U-Net\nEnable SDPA for U-Net\nimport network module: networks.lora\n[Dataset 0]\ncaching latents.\n[Dataset 0]\ncaching latents.\nchecking cache validity...\n  0%|                                                 | 0/11392 [00:00<?, ?it/s]checking cache validity...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11392/11392 [00:00<00:00, 501322.11it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11392/11392 [00:04<00:00, 2613.70it/s]\ncaching latents...\n0it [00:00, ?it/s]\ncreate LoRA network. base dim (rank): 24, alpha: 24\nneuron dropout: p=None, rank dropout: p=None, module dropout: p=None\napply LoRA to Conv2d with kernel size (3,3). dim (rank): 24, alpha: 24.0\ncreate LoRA for Text Encoder 1:\ncreate LoRA network. base dim (rank): 24, alpha: 24\nneuron dropout: p=None, rank dropout: p=None, module dropout: p=None\napply LoRA to Conv2d with kernel size (3,3). dim (rank): 24, alpha: 24.0\ncreate LoRA for Text Encoder 1:\ncreate LoRA for Text Encoder 2:\ncreate LoRA for Text Encoder 2:\ncreate LoRA for Text Encoder: 88 modules.\ncreate LoRA for Text Encoder: 88 modules.\ncreate LoRA for U-Net: 788 modules.\ncreate LoRA for U-Net: 788 modules.\nenable LoRA for text encoder: 88 modules\nenable LoRA for U-Net: 788 modules\nenable LoRA for text encoder: 88 modules\nenable LoRA for U-Net: 788 modules\nprepare optimizer, data loader etc.\nuse Prodigy optimizer | {'decouple': True, 'weight_decay': 0.01, 'betas': [0.9, 0.999], 'd_coef': 2, 'use_bias_correction': True, 'safeguard_warmup': True, 'slice_p': 11}\nUsing decoupled weight decay\nuse Prodigy optimizer | {'decouple': True, 'weight_decay': 0.01, 'betas': [0.9, 0.999], 'd_coef': 2, 'use_bias_correction': True, 'safeguard_warmup': True, 'slice_p': 11}\nUsing decoupled weight decay\nuse LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts | {'min_lr': 1e-09, 'gamma': 0.9, 'd': 0.9, 'first_cycle_max_steps': 3173, 'warmup_steps': 158} as lr_scheduler\noverride steps. steps for 5 epochs is / æŒ‡å®šã‚¨ãƒãƒƒã‚¯ã¾ã§ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°: 4760\nuse LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts | {'min_lr': 1e-09, 'gamma': 0.9, 'd': 0.9, 'first_cycle_max_steps': 3173, 'warmup_steps': 158} as lr_scheduler\nrunning training / å­¦ç¿’é–‹å§‹\n  num train images * repeats / å­¦ç¿’ç”»åƒã®æ•°Ã—ç¹°ã‚Šè¿”ã—å›æ•°: 11392\n  num reg images / æ­£å‰‡åŒ–ç”»åƒã®æ•°: 0\n  num batches per epoch / 1epochã®ãƒãƒƒãƒæ•°: 952\n  num epochs / epochæ•°: 5\n  batch size per device / ãƒãƒƒãƒã‚µã‚¤ã‚º: 6\n  gradient accumulation steps / å‹¾é…ã‚’åˆè¨ˆã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•° = 1\n  total optimization steps / å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°: 4760\nsteps:   0%|                                           | 0/4760 [00:00<?, ?it/s]\nepoch 1/5\nepoch is incremented. current_epoch: 0, epoch: 1\nepoch is incremented. current_epoch: 0, epoch: 1\nsteps:  20%|â–ˆâ–ˆâ–Œ          | 952/4760 [1:14:06<4:56:27,  4.67s/it, avr_loss=0.158]\nsaving checkpoint: /kaggle/working/Loras/hypnohub1024/output/hypnohub1024-01.safetensors\n\nepoch 2/5\nepoch is incremented. current_epoch: 1, epoch: 2\nepoch is incremented. current_epoch: 1, epoch: 2\nsteps:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 1904/4760 [2:28:24<3:42:36,  4.68s/it, avr_loss=0.153]\nsaving checkpoint: /kaggle/working/Loras/hypnohub1024/output/hypnohub1024-02.safetensors\n\nepoch 3/5\nepoch is incremented. current_epoch: 2, epoch: 3\nepoch is incremented. current_epoch: 2, epoch: 3\nsteps:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2856/4760 [3:42:46<2:28:31,  4.68s/it, avr_loss=0.152]\nsaving checkpoint: /kaggle/working/Loras/hypnohub1024/output/hypnohub1024-03.safetensors\n\nepoch 4/5\nepoch is incremented. current_epoch: 3, epoch: 4\nepoch is incremented. current_epoch: 3, epoch: 4\nsteps:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3808/4760 [4:57:08<1:14:17,  4.68s/it, avr_loss=0.152]\nsaving checkpoint: /kaggle/working/Loras/hypnohub1024/output/hypnohub1024-04.safetensors\n\nepoch 5/5\nepoch is incremented. current_epoch: 4, epoch: 5\nepoch is incremented. current_epoch: 4, epoch: 5\nsteps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4760/4760 [6:11:34<00:00,  4.68s/it, avr_loss=0.153]\nsaving checkpoint: /kaggle/working/Loras/hypnohub1024/output/hypnohub1024-05.safetensors\nmodel saved.\nsteps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4760/4760 [6:11:36<00:00,  4.68s/it, avr_loss=0.153]\n[rank0]:[W317 04:07:20.908213192 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### âœ… Done! [Your Lora is in the Kaggle Working directory](https://www.kaggle.com/code/your_kaggle_username/your_notebook_slug/output)\n### There will be several files, you should try the latest version (the file with the largest number next to it)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"!rm -f -r /kaggle/working/Loras/hypnohub1024/output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T21:54:09.461251Z","iopub.execute_input":"2025-03-16T21:54:09.461613Z","iopub.status.idle":"2025-03-16T21:54:09.842592Z","shell.execute_reply.started":"2025-03-16T21:54:09.461585Z","shell.execute_reply":"2025-03-16T21:54:09.841660Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"accelerate_script = os.path.join(kohya_dir, \"venv/bin/accelerate\")\n!{accelerate_script} config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.238188Z","iopub.execute_input":"2025-03-13T13:33:14.238435Z","iopub.status.idle":"2025-03-13T13:33:14.362015Z","shell.execute_reply.started":"2025-03-13T13:33:14.238413Z","shell.execute_reply":"2025-03-13T13:33:14.360986Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: line 1: /kaggle/working/trainer/sd_scripts/venv/bin/accelerate: No such file or directory\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import os\naccelerate = os.path.join(kohya_dir, \"venv/bin/accelerate\")\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import write_basic_config  # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ ÑÑ‚Ñƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ\n\naccelerator = Accelerator()\nconfig = accelerator.distributed_state  # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ\nprint(\"Accelerate Configuration:\")\nprint(config) # Ğ’Ñ‹Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²ÑÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.363938Z","iopub.execute_input":"2025-03-13T13:33:14.364204Z","iopub.status.idle":"2025-03-13T13:33:14.388401Z","shell.execute_reply.started":"2025-03-13T13:33:14.364182Z","shell.execute_reply":"2025-03-13T13:33:14.386966Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-70d0802d44a5>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maccelerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_state\u001b[0m  \u001b[0;31m# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accelerate Configuration:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ğ’Ñ‹Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²ÑÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Accelerator' object has no attribute 'distributed_state'"],"ename":"AttributeError","evalue":"'Accelerator' object has no attribute 'distributed_state'","output_type":"error"}],"execution_count":44},{"cell_type":"markdown","source":"## *ï¸âƒ£ Extras\n\nYou can run these before starting the training.","metadata":{"id":"mBMUJ7BuvNcn"}},{"cell_type":"code","source":"#!rm -f -r /kaggle/working/kaggle\n#!rm -f -r /kaggle/working/Illustrious-XL-v0.1.safetensors\n#!rm -f -r /kaggle/working/sdxl_vae.safetensors\n!rm -f -r /kaggle/working/trainer\n#!rm -f -r /kaggle/working/Loras\n#!rm -f -r /kaggle/working/_logs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:22:43.830625Z","iopub.execute_input":"2025-03-16T18:22:43.831012Z","iopub.status.idle":"2025-03-16T18:22:45.519360Z","shell.execute_reply.started":"2025-03-16T18:22:43.830980Z","shell.execute_reply":"2025-03-16T18:22:45.518009Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### ğŸ“š Multiple folders in dataset\nBelow is a template allowing you to define multiple folders in your dataset. You must include the location of each folder and you can set different number of repeats for each one. To add more folders simply copy and paste the sections starting with `[[datasets.subsets]]`.\n\nWhen enabling this, the number of repeats set in the main cell will be ignored, and the main folder set by the project name will also be ignored.\n\nYou can make one of them a regularization folder by adding `is_reg = true`  \nYou can also set different `keep_tokens`, `flip_aug`, etc.","metadata":{"id":"Wd4916Eu1tb9"}},{"cell_type":"code","source":"custom_dataset = \"\"\"\n[[datasets]]\n\n[[datasets.subsets]]\nimage_dir = \"/kaggle/working/Loras/hypnohub1024/dataset/1\"\nnum_repeats = 1\nkeep_tokens = 1\n\n[[datasets.subsets]]\nimage_dir = \"/kaggle/working/Loras/hypnohub1024/dataset/2\"\nnum_repeats = 1\nkeep_tokens = 2\n\n[[datasets.subsets]]\nimage_dir = \"/kaggle/working/Loras/hypnohub1024/dataset/3\"\nnum_repeats = 1\nkeep_tokens = 3\n\n[[datasets.subsets]]\nimage_dir = \"/kaggle/working/Loras/hypnohub1024/dataset/4\"\nnum_repeats = 1\nkeep_tokens = 4\n\n[[datasets.subsets]]\nimage_dir = \"/kaggle/working/Loras/hypnohub1024/dataset/5\"\nnum_repeats = 1\nkeep_tokens = 5\n\n[[datasets.subsets]]\nimage_dir = \"/kaggle/working/Loras/hypnohub1024/dataset/6\"\nnum_repeats = 1\nkeep_tokens = 6\n\n[[datasets.subsets]]\nimage_dir = \"/kaggle/working/Loras/hypnohub1024/dataset/7\"\nnum_repeats = 1\nkeep_tokens = 7\n\n\"\"\"","metadata":{"id":"Y037lagnJWmn","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:35:03.909606Z","iopub.execute_input":"2025-03-16T18:35:03.909915Z","iopub.status.idle":"2025-03-16T18:35:03.913688Z","shell.execute_reply.started":"2025-03-16T18:35:03.909888Z","shell.execute_reply":"2025-03-16T18:35:03.912753Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"custom_dataset = None","metadata":{"id":"W84Jxf-U2TIU","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.389798Z","iopub.status.idle":"2025-03-13T13:33:14.390080Z","shell.execute_reply":"2025-03-13T13:33:14.389953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@markdown ### ğŸ“‚ Unzip dataset\n#@markdown It's much slower to upload individual files to your Drive, so you may want to upload a zip if you have your dataset in your computer.\nzip = \"/content/drive/MyDrive/hypnohub512.zip\" #@param {type:\"string\"}\nextract_to = \"/content/drive/MyDrive/Loras/hypnohub512/\" #@param {type:\"string\"}\n\nimport os, zipfile\n\nif not os.path.exists('/content/drive'):\n  from google.colab import drive\n  print(\"ğŸ“‚ Connecting to Google Drive...\")\n  drive.mount('/content/drive')\n\nos.makedirs(extract_to, exist_ok=True)\n\nwith zipfile.ZipFile(zip, 'r') as f:\n  f.extractall(extract_to)\n\nprint(\"âœ… Done\")\n","metadata":{"cellView":"form","id":"WDjkp4scvPgE","outputId":"78189c05-c4dc-4779-a27a-defa0828cf00","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.391115Z","iopub.status.idle":"2025-03-13T13:33:14.391506Z","shell.execute_reply":"2025-03-13T13:33:14.391326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@markdown ### ğŸ”¢ Count datasets\n#@markdown Google Drive makes it impossible to count the files in a folder, so this will show you the file counts in all folders and subfolders.\nfolder = \"/content/drive/MyDrive/Loras\" #@param {type:\"string\"}\n\nimport os\nfrom google.colab import drive\n\nif not os.path.exists('/content/drive'):\n    print(\"ğŸ“‚ Connecting to Google Drive...\\n\")\n    drive.mount('/content/drive')\n\ntree = {}\nexclude = (\"_logs\", \"/output\")\nfor i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n  others = len(files) - images - captions\n  path = root[folder.rfind(\"/\")+1:]\n  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n  if tree[path] and others:\n    tree[path] += f\" {others:>4} other files\"\n\npad = max(len(k) for k in tree)\nprint(\"\\n\".join(f\"ğŸ“{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n","metadata":{"cellView":"form","id":"aKWlpsG0jrX3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"79b23acf-cbdd-41aa-ebe0-c89872d23208","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.392480Z","iopub.status.idle":"2025-03-13T13:33:14.392841Z","shell.execute_reply":"2025-03-13T13:33:14.392655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@markdown ### â†ªï¸ Continue\n\n#@markdown Here you can write a path in your Google Drive to load an existing Lora file to continue training on.<p>\n#@markdown **Warning:** It's not the same as one long training session. The epochs start from scratch, and it may have worse results.\ncontinue_from_lora = \"/content/drive/MyDrive/Loras/hypnohub512/output/1234/hypnohub512-step00000600.safetensors\" #@param {type:\"string\"}\nif continue_from_lora and not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n  import os\n  continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n","metadata":{"cellView":"form","id":"-cnM6xM_E6Jx","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.393777Z","iopub.status.idle":"2025-03-13T13:33:14.394165Z","shell.execute_reply":"2025-03-13T13:33:14.393963Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ“ˆ Plot training results\nYou can do this after running the trainer. You don't need this unless you know what you're doing.  \nThe first cell below may fail to load all your logs. Keep trying the second cell until all data has loaded.","metadata":{"id":"YcTjh07x90Ro"}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir={log_folder}/","metadata":{"id":"Z_TRI3eX90Rp","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.394930Z","iopub.status.idle":"2025-03-13T13:33:14.395268Z","shell.execute_reply":"2025-03-13T13:33:14.395149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorboard import notebook\nnotebook.display(port=6006, height=800)","metadata":{"id":"6rM5SLq990Rp","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:33:14.395852Z","iopub.status.idle":"2025-03-13T13:33:14.396186Z","shell.execute_reply":"2025-03-13T13:33:14.396032Z"}},"outputs":[],"execution_count":null}]}